{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj-l6TqdN7ZP"
   },
   "source": [
    "# GenAI Homework 1 Part 1\n",
    "\n",
    "#### Make sure that you read your API Key from a file and submit the api key file with your homework. Only TA and I will have access to your api key to check your homework. I will ask to delete api keys shared with us after checking the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6fd4u4GNnud"
   },
   "source": [
    "**Task 1:** Create an AI assistant that will answer \"What's the temperature outside now\" or \"What's the temperature in Tokio now\" type of questions (location can be any big city). Ask for the location of the user if the location is not in the question. Use OpenAI APIs, [openweathermap](http://api.openweathermap.org/data/2.5/weather) API(it is free), and function calling.\n",
    "\n",
    "* Step 1: Use OpenAI Chat Completions API to get the location of the user if it is not given. If it is given, use function calling for getting weather api parameter(s).\n",
    "* Step 2: Call weather api for the given location.\n",
    "* Step 3: Call Chat Completions API again for processing the response of weather api. Make it to provide short answer like this: \"The temperature in Yerevan is -1.91 degrees Celsius\".\n",
    "* Step 4: Call Chat Completions API again to translate the output of Step 3 into Armenian.\n",
    "* Step 5: Use OpenAI Text to Speech API to create an audio version (mp3) of the output of Step 4.\n",
    "* Step 6: Use one of OpenAIs APIs to create an image based on the output of Step 3 (text to image).\n",
    "* Step 7: Use one of OpenAIs APIs to extract text (if any) from the output of Step 6\n",
    "* Step 8: Create a Chainlit app that will answer the questions mentioned at the beginning of the task and will output the outputs of Steps 3, 4, 5, 6, and 7.\n",
    "\n",
    "Useful links:\n",
    "* [Chainlit App Creation](https://docs.chainlit.io/get-started/pure-python])\n",
    "* [Text, Image, Audio and Video response with Chainlit](https://docs.chainlit.io/api-reference/elements/text)\n",
    "\n",
    "**Check a Chainlit app example at the end of this notebook.**\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "with open('apikey.txt','r') as f:\n",
    "  openai_api_key = f.read()\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Use OpenAI Chat Completions API to get the location of the user if it is not given. If it is given, use function calling for getting weather api parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_current_temperature(city_name, openweathermap_api_key):\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    params = {\n",
    "        'q': city_name,\n",
    "        'appid': openweathermap_api_key,\n",
    "        'units': 'metric'  \n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        temperature = data['main']['temp']\n",
    "        return temperature\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Call weather api for the given location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temperature_response(city_name, client, temperature, openai_api_key):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  \n",
    "        messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are LLM assistant for providing weather.\"},\n",
    "          {\"role\": \"user\", \"content\": f\"Create a short sentence that tells the current temperature in {city_name}, which is {temperature:.2f} degrees Celsius\"}\n",
    "        ]\n",
    "    ) \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Call Chat Completions API again for processing the response of weather api. Make it to provide short answer like this: \"The temperature in Yerevan is -1.91 degrees Celsius\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_city_name(user_query):\n",
    "    cleaned_query = re.sub(r'[^\\w\\s]', '', user_query)\n",
    "    \n",
    "    key_words = ['in', 'for', 'at', 'today', 'temperature']\n",
    "    \n",
    "    words = cleaned_query.split()\n",
    "    \n",
    "    for key in key_words:\n",
    "        if key in words:\n",
    "            key_index = words.index(key)\n",
    "            if key_index + 1 < len(words):\n",
    "                return words[key_index + 1]\n",
    "                \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_temperature_query(user_query, client, openweathermap_api_key, openai_api_key):\n",
    "    city_name = extract_city_name(user_query)\n",
    "    if city_name:\n",
    "        temperature = get_current_temperature(city_name, openweathermap_api_key)\n",
    "        \n",
    "        if temperature is not None:\n",
    "            response_message = generate_temperature_response(city_name, client, temperature, openai_api_key)\n",
    "            return response_message  \n",
    "        else:\n",
    "            return f\"Could not retrieve the temperature for {city_name}.\"\n",
    "    else:\n",
    "        return \"Please specify the city for which you'd like to know the temperature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Yeravan is 9.09 degrees Celsius.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "user_query = \"What's the temperature in Yerevan now?\"\n",
    "openweathermap_api_key = \"3b3ab17dc8c57469292f3f52b3278c0f\"\n",
    "english_response = process_temperature_query(user_query, client, openweathermap_api_key, openai_api_key)\n",
    "english_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Call Chat Completions API again to translate the output of Step 3 into Armenian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cjSkXMgXUnRE"
   },
   "outputs": [],
   "source": [
    "def generate_response_in_armenian(english_response, client, openai_api_key):\n",
    "    translation_prompt = f\"Translate the following sentence into Armenian: '{english_response}'\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant capable of translating English to Armenian.\"},\n",
    "            {\"role\": \"user\", \"content\": translation_prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Երևանի ընթացիկ ջերմաստիճանը կազմում է 9.09 աստիճան Ցելսիուս։ '\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "armenian_response = generate_response_in_armenian(english_response, client, openai_api_key)\n",
    "armenian_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Use OpenAI Text to Speech API to create an audio version (mp3) of the output of Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_speech_and_save(input_text, client, output_file=\"output.mp3\"):\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1-hd\",  \n",
    "        voice=\"alloy\",     \n",
    "        input=input_text   \n",
    "    )\n",
    "\n",
    "    response.stream_to_file(output_file)\n",
    "    print(f\"Audio content successfully saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content successfully saved to 'Temperature_in_Armenian.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0c/j4c53d0102lcr2b5rk247yrm0000gn/T/ipykernel_13297/4145884099.py:8: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(output_file)\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "convert_text_to_speech_and_save(armenian_response, client, \"Temperature_in_Armenian.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use one of OpenAIs APIs to create an image based on the output of Step 3 (text to image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_and_get_url(prompt, client, size=\"1024x1024\", quality=\"hd\", n=1):\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",  \n",
    "        prompt=prompt,     \n",
    "        size=size,         \n",
    "        quality=quality,   \n",
    "        n=n                \n",
    "    )\n",
    "\n",
    "    image_url = response.data[0].url  \n",
    "    return image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-ej7auLzDqe4O6uOw5EHRgwBR/user-ZFHofgMljsoZMuNDhY1cl8h7/img-hIeOtyzsE2iQSzI0UplsZMIz.png?st=2024-03-11T08%3A49%3A07Z&se=2024-03-11T10%3A49%3A07Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-03-10T12%3A04%3A18Z&ske=2024-03-11T12%3A04%3A18Z&sks=b&skv=2021-08-06&sig=za%2BgIbFatTFTJdXROoUaM7Bp6u9/psyySmT8wgvOwI8%3D'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "image_url = generate_image_and_get_url(english_response, client)\n",
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Use one of OpenAIs APIs to extract text (if any) from the output of Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_with_text(image_url, client, question=\"What’s in this image?\", model=\"gpt-4-vision-preview\", max_tokens=300):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_url,\n",
    "                            \"detail\": \"high\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image features a cityscape with a prominent electronic billboard showing the temperature and weather for Yerevan, the capital city of Armenia. The temperature is displayed as 9.09 degrees Celsius along with a symbol indicating sunny or partly sunny weather. The architecture of the buildings suggests a blend of modern and historical styles, typical of Yerevan, which combines contemporary structures with traditional Armenian designs. The city is set against a backdrop of mountains, likely representing Mount Ararat, which is a significant symbol for Armenians and visible from many parts of Yerevan on clear days.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "response_content = analyze_image_with_text(image_url, client)\n",
    "response_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tUW4_UbVqrZ"
   },
   "source": [
    "**Task 2:** Use a python library to download a short video from YouTube (e.g. https://github.com/pytube/pytube ), transcribe the text of the video with OpenAIs Whisper API, and use OpenAI's Moderation API to check it. Print the transcribed text and Moderation API response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a python library to download a short video from YouTube (e.g. https://github.com/pytube/pytube )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wRomsBy3UnXl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tigrangaplanyan/Downloads/Tigran_Gaplanyan_HW1/What is Artificial Intelligence  Quick Learner.mp4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "YouTube('https://www.youtube.com/watch?v=c0m6yaGlZh4').streams.first().download()\n",
    "yt = YouTube('https://www.youtube.com/watch?v=c0m6yaGlZh4')\n",
    "yt.streams \\\n",
    "  .filter(progressive=True, file_extension='mp4') \\\n",
    "  .order_by('resolution') \\\n",
    "  .desc() \\\n",
    "  .first() \\\n",
    "  .download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe the text of the video with OpenAIs Whisper API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/speech/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When you hear the term artificial intelligence, what comes to mind? Superpowered robots? Hyperintelligent devices? Science fiction has familiarized the world with the concept, but outside of Hollywood, what is artificial intelligence and what can AI actually do? For starters, AI involves using computers to do things that usually require human intelligence. Humans can see with their eyes and process what they see. We can understand our environments and move around within them. Our brains have the ability to see patterns, and we have the ability to understand spoken language. Artificial intelligence is a broad branch of computer science that includes many different terms. In order for computers to achieve these capabilities, they require lots and lots of data. Artificial intelligence sets allow AI algorithms to identify patterns, make predictions, and recommend actions. Artificial intelligence is already all around us, but today's best AI still can't compete with the human brain in some ways. For example, in 2016, the computer program AlphaGo defeated a legendary professional Go player. But if you asked that same computer to drive a car or walk or even play monopoly, it wouldn't be able to do so on its own. AI's computing power is massive, but our human brains are able to tackle a much wider set of data and methods.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"What is Artificial Intelligence  Quick Learner.mp4\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI's Moderation API to check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'modr-91WZqoFhzYCuiwoBpx2cEOP55xN2S',\n",
       " 'model': 'text-moderation-007',\n",
       " 'results': [{'flagged': False,\n",
       "   'categories': {'sexual': False,\n",
       "    'hate': False,\n",
       "    'harassment': False,\n",
       "    'self-harm': False,\n",
       "    'sexual/minors': False,\n",
       "    'hate/threatening': False,\n",
       "    'violence/graphic': False,\n",
       "    'self-harm/intent': False,\n",
       "    'self-harm/instructions': False,\n",
       "    'harassment/threatening': False,\n",
       "    'violence': False},\n",
       "   'category_scores': {'sexual': 1.858225687101367e-06,\n",
       "    'hate': 0.0003464908804744482,\n",
       "    'harassment': 0.0012262610252946615,\n",
       "    'self-harm': 9.81941905564554e-09,\n",
       "    'sexual/minors': 3.648816360168894e-08,\n",
       "    'hate/threatening': 6.078220593508377e-08,\n",
       "    'violence/graphic': 3.0797647923463956e-05,\n",
       "    'self-harm/intent': 3.2770035574003487e-08,\n",
       "    'self-harm/instructions': 1.2896180123789236e-07,\n",
       "    'harassment/threatening': 9.040778422786389e-06,\n",
       "    'violence': 8.514392538927495e-05}}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.openai.com/v1/moderations\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f'Bearer {openai_api_key}'  # Replace YOUR_OPENAI_API_KEY with your actual API key\n",
    "}\n",
    "data = {\n",
    "    \"input\": result[\"text\"]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJGFkzMldY0p"
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7lNFh5gg6Si"
   },
   "outputs": [],
   "source": [
    "# Chainlit app example with text, image and audio\n",
    "# Write below code in app.py and run it locally\n",
    "import chainlit as cl\n",
    "from openai import OpenAI\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    # Your custom logic goes here...\n",
    "    image = cl.Image(path=\"files/nature.png\", name=\"Nature\", display=\"inline\")\n",
    "    audio = cl.Audio(name=\"output.mp3\", path=\"files/output.mp3\", display=\"inline\")\n",
    "    elements = [image, audio]\n",
    "\n",
    "\n",
    "    with open('apikey.txt','r') as f:\n",
    "        OPENAI_API_KEY = f.read()\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                 model=\"gpt-4-1106-preview\",\n",
    "                  response_format={ \"type\": \"text\" }, messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are LLM assistant that provides short answers.\"},\n",
    "    {\"role\": \"user\", \"content\": message.content}\n",
    "  ]\n",
    ")\n",
    "\n",
    "    f_response = response.choices[0].message.content\n",
    "\n",
    "# Send a response back to the user\n",
    "    await cl.Message(\n",
    "        content=f\"Received: {f_response}\",\n",
    "\telements = elements,\n",
    "    ).send()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
